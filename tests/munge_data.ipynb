{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NOTE: most of the commented-out code was executed in a terminal and/or \n",
      "# datasets.py and subsequently pasted in here; consequently, it probably\n",
      "# won't work out of the box.\n",
      "\n",
      "import os\n",
      "import numpy as np\n",
      "from scipy.io import loadmat\n",
      "\n",
      "# %load_ext autoreload\n",
      "\n",
      "%aimport utils\n",
      "%autoreload 1  # 1 = autoreload only modules imported with %aimport\n",
      "# %autoreload 2\n",
      "\n",
      "DATA_DIR = os.path.expanduser(\"~/Desktop/datasets/nn-search/\")\n",
      "\n",
      "contig = np.ascontiguousarray\n",
      "join = os.path.join\n",
      "\n",
      "def save_to_data_dir(relative_path, X):\n",
      "    np.save(join(DATA_DIR, relative_path), np.ascontiguousarray(X))\n",
      "\n",
      "def load_from_data_dir(relative_path):\n",
      "    return np.load(join(DATA_DIR, relative_path))\n",
      "\n",
      "def loadmat_from_data_dir(relative_path):\n",
      "    return loadmat(join(DATA_DIR, relative_path))\n",
      "\n",
      "def read_yael_vecs(path, c_contiguous=True, limit_rows=-1, dtype=None):\n",
      "    dim = np.fromfile(path, dtype=np.int32, count=2)[0]\n",
      "    print \"vector length = {}\".format(dim)\n",
      "\n",
      "    if dtype is None:\n",
      "        if 'fvecs' in path:\n",
      "            dtype = np.float32\n",
      "        elif 'ivecs' in path:\n",
      "            dtype = np.int32\n",
      "        elif 'bvecs' in path:\n",
      "            dtype = np.uint8\n",
      "        else:\n",
      "            raise ValueError(\"couldn't infer dtype from path {}\".format(path))\n",
      "    itemsize = np.dtype(dtype).itemsize\n",
      "\n",
      "    assert dim > 0\n",
      "    assert itemsize in (1, 2, 4)\n",
      "\n",
      "    cols_for_dim = 4 // itemsize\n",
      "    row_size_bytes = 4 + dim * itemsize\n",
      "    row_size_elems = row_size_bytes // itemsize\n",
      "    limit = int(limit_rows) * row_size_elems if limit_rows > 0 else -1\n",
      "\n",
      "    fv = np.fromfile(path, dtype=dtype, count=limit)\n",
      "    fv = fv.reshape((-1, row_size_elems))\n",
      "\n",
      "    if not all(fv.view(np.int32)[:, 0] == dim):\n",
      "        raise IOError(\"Non-uniform vector sizes in \" + path)\n",
      "\n",
      "    fv = fv[:, cols_for_dim:]\n",
      "\n",
      "    if c_contiguous:\n",
      "        fv = fv.copy()\n",
      "    return fv\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gist1M"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # ------------------------ clean up gist (now broken)\n",
      "# path = Paths.DATA_DIR + '/gist/gist_base.fvecs'\n",
      "# path = Paths.DATA_DIR + '/gist/gist_learn.fvecs'\n",
      "# path = Paths.DATA_DIR + '/gist/gist_queries.fvecs'\n",
      "# path = Paths.DATA_DIR + '/gist/gist_groundtruth.ivecs'\n",
      "# out_path = Paths.GIST_100\n",
      "# out_path = Paths.GIST_200\n",
      "# out_path = Paths.GIST_TRAIN\n",
      "# out_path = Paths.GIST_QUERIES\n",
      "# out_path = Paths.GIST_TRUTH\n",
      "# X = read_yael_vecs(path)[:100000]\n",
      "# X = read_yael_vecs(path)[:200000]\n",
      "# X = read_yael_vecs(path)\n",
      "# X = read_yael_vecs(path, dtype=np.int32)\n",
      "# print X[:2]\n",
      "# print X.shape\n",
      "# np.save(out_path, X)\n",
      "# truth_dir = data_dir + 'gnd/'\n",
      "# # truth_idxs_files = ['idx_1M', 'idx_10M', 'idx_100M']\n",
      "# truth_idxs_files = ['idx_1000M']\n",
      "# for f in truth_idxs_files:\n",
      "#     path = truth_dir + f + '.ivecs'\n",
      "#     out_path = out_dir + f + '.npy'\n",
      "#     print \"unpacking {} to {}\".format(path, out_path)\n",
      "#     X = read_yael_vecs(path)\n",
      "#     print X.shape\n",
      "#     np.save(out_path, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sift1M"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ------------------------ clean up sift1m\n",
      "# data_dir = '/Volumes/MacHDD/datasets/sift1m/'\n",
      "# out_dir = '/Volumes/MacSSD_OS/Users/davis/Desktop/datasets/sift1m/'\n",
      "# for fname in os.listdir(data_dir):\n",
      "#     in_path = data_dir + fname\n",
      "#     out_path = out_dir + fname.split('.')[0] + '.npy'\n",
      "#     print \"unpacking {} to {}\".format(in_path, out_path)\n",
      "#     X = read_yael_vecs(in_path)\n",
      "#     print X.shape\n",
      "#     np.save(out_path, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Deep1M"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # ------------------------ clean up Deep1M\n",
      "# data_dir = os.path.expanduser('~/Desktop/datasets/nn-search/deep1M-raw/')\n",
      "# out_dir = os.path.expanduser('~/Desktop/datasets/nn-search/deep1M/')\n",
      "# print \"in dir, out dir:\", data_dir, out_dir\n",
      "# for fname in os.listdir(data_dir):\n",
      "#     in_path = data_dir + fname\n",
      "#     out_path = out_dir + fname.split('.')[0] + '.npy'\n",
      "#     print \"unpacking {} to {}\".format(in_path, out_path)\n",
      "#     X = read_yael_vecs(in_path)\n",
      "#     print X.shape\n",
      "#     np.save(out_path, X)\n",
      "\n",
      "X_train = load_from_data_dir('deep1m/deep1M_learn.npy')\n",
      "print X_train.shape\n",
      "print np.isfortran(X_train) # false\n",
      "\n",
      "Q = load_from_data_dir('deep1m/deep1M_queries.npy')\n",
      "print Q.shape\n",
      "print np.isfortran(Q) # false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(300000, 256)\n",
        "False\n",
        "(1000, 256)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "truth_train = utils.compute_true_knn(X_train, Q)\n",
      "print truth_train.shape\n",
      "print truth_train.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing top k for query block 0 (queries 0-128)...\n",
        "computing top k for query block 5 (queries 640-768)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1000, 1000)\n",
        "int32\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_to_data_dir('deep1m/deep1M_truth_train.npy', truth_train.astype(np.int32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = load_from_data_dir('deep1m/deep1M_base.npy')\n",
      "print X_test.shape\n",
      "print np.isfortran(X_test) # false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000000, 256)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "truth_test = utils.compute_true_knn(X_test, Q)\n",
      "print truth_test.shape\n",
      "print truth_test.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing top k for query block 0 (queries 0-128)...\n",
        "computing top k for query block 5 (queries 640-768)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1000, 1000)\n",
        "int32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_to_data_dir('deep1m/deep1M_truth_test.npy', truth_test.astype(np.int32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Convnet 1M"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # ------------------------ clean up Convnet1M\n",
      "# import numpy as np\n",
      "# from scipy.io import loadmat\n",
      "# d = loadmat('features_m_128.mat')\n",
      "# contig = np.ascontiguousarray\n",
      "# savedir = '../convnet1m/'\n",
      "# np.save(savedir + 'convnet_train.npy', contig(d['feats_m_128_train']))\n",
      "# np.save(savedir + 'convnet_test.npy', contig(d['feats_m_128_test']))\n",
      "# np.save(savedir + 'convnet_base.npy', contig(d['feats_m_128_base']))\n",
      "\n",
      "X_train = load_from_data_dir('convnet1m/convnet_train.npy')\n",
      "print np.isfortran(X_train) # false\n",
      "print X_train.shape\n",
      "# if np.isfortran(X_train):\n",
      "#     save_to_data_dir('convnet1m/convnet_train.npy', X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "False\n",
        "(100000, 128)\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = load_from_data_dir('convnet1m/convnet_queries.npy')\n",
      "print Q.shape\n",
      "print np.isfortran(Q)\n",
      "# save_to_data_dir('convnet1m/convnet_queries.npy', Q)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10000, 128)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print Q[:20, :20]  yep, looks like relu activations\n",
      "# dists = utils.sq_dists_to_vectors(Q, X_train)\n",
      "# print dists.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nqueries = Q.shape[0]\n",
      "k = 1000\n",
      "truth_train = np.full((nqueries, k), -999, dtype=np.int32)\n",
      "print nqueries\n",
      "# for i in range(100):\n",
      "for i in range(nqueries):\n",
      "    if i % 1000 == 0:\n",
      "        print \"computing top k for query {}...\".format(i)\n",
      "    truth_train[i, :] = utils.top_k_idxs(dists[i, :], k)\n",
      "print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000\n",
        "computing top k for query 0...\n",
        "computing top k for query 500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 1000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 1500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 2000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 2500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 3000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 3500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 4000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 4500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 5000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 5500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 6000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 6500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 7000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 7500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 8000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 8500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 9000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 9500..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert not np.any(truth_train == -999)\n",
      "save_to_data_dir('convnet1m/truth_train.npy', truth_train.astype(np.int32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = load_from_data_dir('convnet1m/convnet_test.npy')\n",
      "print X_test.shape\n",
      "print np.isfortran(X_test)\n",
      "# if np.isfortran(X_test):\n",
      "#     save_to_data_dir('convnet1m/convnet_test.npy', X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1000000, 128)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# # truth_test = utils.compute_true_knn(X_test[:1000], Q[:130], block_sz=128)\n",
      "# truth_test = utils.compute_true_knn(X_test, Q)\n",
      "# print truth_test.shape\n",
      "# print truth_test.dtype"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "computing top k for query block 0 (queries 0-128)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 5 (queries 640-768)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 10 (queries 1280-1408)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 15 (queries 1920-2048)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 20 (queries 2560-2688)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 25 (queries 3200-3328)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 30 (queries 3840-3968)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 35 (queries 4480-4608)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 40 (queries 5120-5248)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 45 (queries 5760-5888)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 50 (queries 6400-6528)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 55 (queries 7040-7168)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 60 (queries 7680-7808)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 65 (queries 8320-8448)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 70 (queries 8960-9088)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 75 (queries 9600-9728)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000\n",
        "WARNING: sq_dists_to_vectors: attempting to create a matrixof size 128000000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(10000, 1000)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "int32\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert not np.any(truth_test == -999)\n",
      "save_to_data_dir('convnet1m/truth_test.npy', truth_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_to_data_dir('convnet1m/convnet_test_100k.npy', X_test[100000, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Deep1B"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ------------------------ clean up deep1b\n",
      "# data_dir = '/Volumes/MacHDD/datasets/deep1b/'\n",
      "# out_dir = '/Volumes/MacSSD_OS/Users/davis/Desktop/datasets/deep1b/'\n",
      "\n",
      "# # expected_cols = 96\n",
      "# # equivalent_elements_in_first_1M = int(1e6) * (1 + expected_cols)\n",
      "# arrays = []\n",
      "# # arrays.append(('deep1B_queries.fvecs', 'deep_queries.npy', -1))\n",
      "# # arrays.append(('deep1B_groundtruth.ivecs', 'deep_true_nn_idxs.npy', -1))\n",
      "# # arrays.append(('deep10M.fvecs', 'deep_1M.npy', 1e6))\n",
      "# arrays.append(('deep10M.fvecs', 'deep_10M.npy', -1))\n",
      "# for in_file, out_file, limit in arrays:\n",
      "#     in_path = data_dir + in_file\n",
      "#     out_path = out_dir + out_file\n",
      "#     X = read_yael_vecs(in_path, limit_rows=limit)\n",
      "#     print \"unpacking {} to {}\".format(in_path, out_path)\n",
      "#     print X.shape\n",
      "#     np.save(out_path, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "LabelMe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ------------------------ clean up LabelMe\n",
      "# >>> from scipy.io import loadmat\n",
      "# >>> d = loadmat('LabelMe_gist.mat')\n",
      "# >>> for k, v in d.iteritems():\n",
      "# ...     try:\n",
      "# ...             print k, v.shape\n",
      "# ...     except:\n",
      "# ...             pass\n",
      "# ...\n",
      "# gist (22019, 512)\n",
      "# img (32, 32, 3, 22019)\n",
      "# nmat (1000, 1000, 20)\n",
      "# __header__ param (1, 1)\n",
      "# __globals__ seg (32, 32, 22019)\n",
      "# names (1, 3597)\n",
      "# DistLM (22019, 22019)\n",
      "# __version__ ndxtrain (1, 20019)\n",
      "# ndxtest (1, 2000)\n",
      "#\n",
      "# okay, no idea what most of these are even with the readme...\n",
      "#\n",
      "# >>> np.save('labelme_train_idxs', d['ndxtrain']) # training data idxs\n",
      "# >>> np.save('labelme_test_idxs', d['ndxtest'])   # test data idxs\n",
      "# >>> np.save('labelme_all_gists', d['gist'])     # actual gist descriptors\n",
      "\n",
      "X = load_from_data_dir('labelme/labelme_all_gists.npy')\n",
      "np.isfortran(X)  # this is true; suggests lots of other stuff also F order"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_idxs = load_from_data_dir('labelme/labelme_train_idxs.npy').ravel() - 1 # one-indexed\n",
      "X_train = X[train_idxs, :]\n",
      "# save_to_data_dir('labelme/labelme_train.npy', X_train)\n",
      "\n",
      "test_idxs = load_from_data_dir('labelme/labelme_test_idxs.npy').ravel() - 1 # one-indexed\n",
      "X_test = X[test_idxs, :]\n",
      "# save_to_data_dir('labelme/labelme_test.npy', X_test)\n",
      "\n",
      "print train_idxs.shape\n",
      "print test_idxs.shape\n",
      "\n",
      "train_set = set(list(train_idxs))\n",
      "test_set = set(list(test_idxs))\n",
      "assert len(train_set.intersection(test_set)) == 0\n",
      "assert len(train_set) + len(test_set) == len(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20019,)\n",
        "(2000,)\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = loadmat_from_data_dir('labelme/LabelMe_gist.mat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print d.keys()\n",
      "print d['gist'].shape\n",
      "print d['seg'].shape\n",
      "print d['nmat'].shape\n",
      "print d['DistLM'].shape\n",
      "print (d['DistLM'] != 0).sum() / 22019\n",
      "\n",
      "# alright, ya, there don't seem to be predefined queries or true neighbors for them"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['gist', 'img', 'nmat', '__header__', 'param', '__globals__', 'seg', 'names', 'DistLM', '__version__', 'ndxtrain', 'ndxtest']\n",
        "(22019, 512)\n",
        "(32, 32, 22019)\n",
        "(1000, 1000, 20)\n",
        "(22019, 22019)\n",
        "836"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dists = utils.sq_dists_to_vectors(X, X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "truth = np.argsort(dists, axis=1)[:1000]\n",
      "save_to_data_dir('labelme/labelme_truth.npy', truth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = load_from_data_dir('labelme/labelme_train.npy')\n",
      "print X_train.shape\n",
      "print np.isfortran(X_train) # false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(20019, 512)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test = load_from_data_dir('labelme/labelme_test.npy')\n",
      "print X_test.shape\n",
      "print np.isfortran(X_test)  # false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(2000, 512)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "MNIST"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ------------------------ clean up mnist\n",
      "import mnist\n",
      "loader = mnist.MNIST(DATA_DIR + 'mnist/')\n",
      "X_train, Y_train = loader.load_training()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_2d = np.concatenate(X_train, axis=0)\n",
      "X_train_2d = X_train_2d.reshape((-1, 784))\n",
      "print mnist.MNIST.display(X_train[3])\n",
      "print Y_train[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "............................\n",
        "............................\n",
        "............................\n",
        "............................\n",
        "............................\n",
        "...................@@.......\n",
        "..................@@@.......\n",
        "..................@@@.......\n",
        ".................@@@........\n",
        "................@@@.........\n",
        "................@@..........\n",
        "...............@@@..........\n",
        "..............@@@...........\n",
        "..............@@............\n",
        ".............@@.............\n",
        "............@@@.............\n",
        "............@@@.............\n",
        "...........@@@..............\n",
        "..........@@@...............\n",
        "..........@@@...............\n",
        "..........@@@...............\n",
        ".........@@@................\n",
        ".........@@@................\n",
        ".........@@@................\n",
        "..........@@................\n",
        "............................\n",
        "............................\n",
        "............................\n",
        "5\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(Y_train)\n",
      "save_to_data_dir('mnist/X_train.npy', X_train_2d.astype(np.float32))\n",
      "save_to_data_dir('mnist/Y_train.npy', Y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'array.array'>\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test, Y_test = loader.load_testing()\n",
      "X_test_2d = np.concatenate(X_test, axis=0)\n",
      "X_test_2d = X_test_2d.reshape((-1, 784))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_to_data_dir('mnist/X_test.npy', X_test_2d.astype(np.float32))\n",
      "save_to_data_dir('mnist/Y_test.npy', Y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print X_test_2d.shape\n",
      "print np.min(X_test)\n",
      "print np.max(X_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(10000, 784)\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "255"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# like other papers, treat test set as queries, train as database\n",
      "truth = utils.compute_true_knn(X_train_2d, X_test_2d)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing true euclidean distances...\n",
        "computing top k for query 0..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 1000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 2000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 3000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 4000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 5000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 6000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 7000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 8000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query 9000..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save_to_data_dir('mnist/truth_Q=train_X=test.npy', truth)\n",
      "save_to_data_dir('mnist/truth_Q=test_X=train.npy', truth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Glove"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.loadtxt(DATA_DIR + 'glove/glove.txt')\n",
      "print X.shape\n",
      "print np.isfortran(X) # false\n",
      "\n",
      "Q, X_test = X[:10000], X[10000:]\n",
      "save_to_data_dir('glove/glove_queries.npy', Q)\n",
      "save_to_data_dir('glove/glove_test.npy', X_test)\n",
      "\n",
      "# Q = load_from_data_dir('deep1m/deep1M_queries.npy')\n",
      "# print Q.shape\n",
      "# print np.isfortran(Q) # false"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1193514, 100)\n",
        "False\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "truth = utils.compute_true_knn(X_test, Q)\n",
      "print truth.dtype\n",
      "print truth.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing top k for query block 0 (queries 0-128)...\n",
        "computing top k for query block 5 (queries 640-768)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 10 (queries 1280-1408)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 15 (queries 1920-2048)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 20 (queries 2560-2688)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 25 (queries 3200-3328)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 30 (queries 3840-3968)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 35 (queries 4480-4608)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 40 (queries 5120-5248)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 45 (queries 5760-5888)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 50 (queries 6400-6528)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 55 (queries 7040-7168)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 60 (queries 7680-7808)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 65 (queries 8320-8448)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 70 (queries 8960-9088)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "computing top k for query block 75 (queries 9600-9728)..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "int32"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(10000, 1000)\n"
       ]
      }
     ],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "save_to_data_dir('glove/truth.npy', truth)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    }
   ],
   "metadata": {}
  }
 ]
}